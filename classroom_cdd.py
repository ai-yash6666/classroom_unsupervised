# -*- coding: utf-8 -*-
"""unllllllll1111111.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uJGCM-aLA7i7Q_6KRHb9g3CqJjguPA_W
"""

import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from zipfile import ZipFile
from google.colab import drive

# Step 1: Mount Google Drive
drive.mount('/content/drive')

# Step 2: Update the zip file path to your Google Drive location
data_path = '/content/drive/MyDrive/Classroom.zip'

# Step 3: Extract the dataset to /content/
extract_path = '/content/Classroom'  # You can change this if you want

with ZipFile(data_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)
    print('The dataset has been extracted.')

# Step 4: Set image size and dataset path
IMG_SIZE = 256
BATCH_SIZE = 32
DATASET_PATH = extract_path  # Path to extracted folder

# ImageDataGenerator with validation split
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2  # 20% validation split
)

train_generator = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

val_generator = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    shuffle=False
)

# CNN Model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(train_generator.num_classes, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Show model summary
model.summary()

# Early stopping callback
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(
    train_generator,
    epochs=20,
    validation_data=val_generator,
    callbacks=[early_stop]
)

# Save the model
model.save("/content/Classroom2_Model.h5")

print("Model saved successfully.")

# Save the model
model.save("/content/Classroom2_Model.h5")

print("Model saved successfully.")

from google.colab import files
files.download('Classroom2_Model.h5')

import numpy as np
import cv2
import tensorflow as tf

# Load model
model = tf.keras.models.load_model("Classroom2_Model.h5")

# Define correct class order based on your training
classes = ['Raising_Hand','Looking_Forward','Sleeping' ]

# Test image path
image_path = "/content/sleepttttttt.jpg"  # CHANGE THIS for each test

# Preprocess the image
IMG_SIZE = 256
img = cv2.imread(image_path)

# Check if the image was loaded successfully
if img is None:
    print(f"Error: Could not load image from {image_path}. Please check the path and file.")
else:
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
    img = img / 255.0  # Only if you used rescale=1./255 during training
    img = np.expand_dims(img, axis=0)

    # Predict
    predictions = model.predict(img)
    print("Raw prediction scores:", predictions)
    print("Predicted class:", classes[np.argmax(predictions)])

print(train_generator.class_indices)

print(train_generator.class_indices)
# Example output: {'Raising_Hand': 0, 'Looking_Forward': 1, 'Sleeping': 2}

# Get correct class order
class_indices = train_generator.class_indices
classes = list(class_indices.keys())
print("Class order:", classes)

print("Class indices:", train_generator.class_indices)
print("Number of training samples:", train_generator.samples)
print("Number of validation samples:", val_generator.samples)

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Get class indices
class_indices = train_generator.class_indices
num_classes = len(class_indices)

# Get labels for all training samples
train_labels = train_generator.classes

# Compute class weights
class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)
class_weights_dict = dict(enumerate(class_weights))

print("Class weights:", class_weights_dict)

# prompt: want to generate a report of this model

import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

print("Class indices:", train_generator.class_indices)
print("Number of training samples:", train_generator.samples)
print("Number of validation samples:", val_generator.samples)
print("Class weights:", class_weights_dict)
print("Model summary:")
model.summary()

# prompt: create a confuaion matrix

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# ... (your existing code) ...

# Predict on validation data
Y_pred = model.predict(val_generator)
y_pred = np.argmax(Y_pred, axis=1)

# Get true labels
Y_true = val_generator.classes

# Compute confusion matrix
cm = confusion_matrix(Y_true, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=classes, yticklabels=classes)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

# prompt: create a code for creating a precision recall

import numpy as np
from sklearn.metrics import precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt

# ... (your existing code) ...

# Predict on validation data
Y_pred = model.predict(val_generator)
y_pred = np.argmax(Y_pred, axis=1)

# Get true labels
Y_true = val_generator.classes

# ... (rest of your existing code)

# Calculate precision and recall for each class
precision = dict()
recall = dict()
average_precision = dict()
for i in range(num_classes):
    precision[i], recall[i], _ = precision_recall_curve(
        (Y_true == i).astype(int), Y_pred[:, i]
    )
    average_precision[i] = average_precision_score(
        (Y_true == i).astype(int), Y_pred[:, i]
    )

# Plot precision-recall curve for each class
plt.figure(figsize=(10, 8))
for i in range(num_classes):
    plt.plot(recall[i], precision[i], label=f'Class {classes[i]} (AP = {average_precision[i]:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve for Each Class')
plt.legend(loc="lower left")
plt.show()

# Calculate micro-averaged precision-recall curve and AP
precision["micro"], recall["micro"], _ = precision_recall_curve(
    Y_true, Y_pred.ravel()
)
average_precision["micro"] = average_precision_score(Y_true, Y_pred, average="micro")

print(f"Micro-averaged average precision: {average_precision['micro']:.2f}")

# prompt: craete f1 scoree and other related things

import numpy as np
from sklearn.metrics import f1_score

# ... (your existing code) ...

# Predict on validation data
Y_pred = model.predict(val_generator)
y_pred = np.argmax(Y_pred, axis=1)

# Get true labels
Y_true = val_generator.classes

# Calculate F1 score
f1 = f1_score(Y_true, y_pred, average='weighted')  # Use 'weighted' for multi-class
print(f"Weighted F1 Score: {f1}")

# Calculate F1 score for each class (macro-averaged)
f1_macro = f1_score(Y_true, y_pred, average='macro')
print(f"Macro-averaged F1 Score: {f1_macro}")


# Calculate F1 score for each class (micro-averaged)
f1_micro = f1_score(Y_true, y_pred, average='micro')
print(f"Micro-averaged F1 Score: {f1_micro}")

# Calculate F1 score for each class individually
f1_per_class = f1_score(Y_true, y_pred, average=None)
for i, f1_class in enumerate(f1_per_class):
    print(f"F1 Score for class {classes[i]}: {f1_class}")

